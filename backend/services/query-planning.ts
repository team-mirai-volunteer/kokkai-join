// Query planning service for Kokkai RAG system

import { getOpenAIClient, resolveModel } from "../config/openai.ts";
import type { QueryPlan } from "../types/kokkai.ts";
import { createQueryPlanPrompt, getQueryPlanSystemPrompt } from "../utils/prompt.ts";
import { AICacheManager } from "../utils/ai-cache-manager.ts";

/**
 * ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã€‚
 *
 * - å½¹å‰²: ãƒ¦ãƒ¼ã‚¶è³ªå•ã‚’è§£æã—ã€æ¤œç´¢ã«é©ã—ãŸã‚µãƒ–ã‚¯ã‚¨ãƒªã‚„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆè©±è€…/æ”¿å…š/æœŸé–“ãªã©ï¼‰ã‚’æŠ½å‡ºã€‚
 * - æœ¬å®Ÿè£…: OpenAI çµŒç”±ã§é¸æŠã—ãŸ LLM ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã—ã¦ JSON å½¢å¼ã®ãƒ—ãƒ©ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ã€‚
 * - ã‚­ãƒ£ãƒƒã‚·ãƒ¥: AIå¿œç­”ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã€ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å†åˆ©ç”¨å¯èƒ½ã€‚
 */
export class QueryPlanningService {
  private cacheManager: AICacheManager;

  constructor(cacheManager?: AICacheManager) {
    this.cacheManager = cacheManager || new AICacheManager();
  }

  /** ãƒ¦ãƒ¼ã‚¶è³ªå•ã‹ã‚‰ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ï¼ˆã‚µãƒ–ã‚¯ã‚¨ãƒªç­‰ï¼‰ã‚’ç”Ÿæˆ */
  async createQueryPlan(userQuestion: string): Promise<QueryPlan> {
    console.log("ğŸ§  Planning query strategy...");

    const userPrompt = createQueryPlanPrompt(userQuestion);
    const cacheInput = {
      userQuestion,
      userPrompt,
      systemPrompt: getQueryPlanSystemPrompt(),
      model: resolveModel("query_planning"),
    };

    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    const cachedPlan = await this.cacheManager.load<QueryPlan>(
      "query-planning",
      cacheInput,
    );

    if (cachedPlan) {
      return cachedPlan;
    }

    // ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
    if (this.cacheManager.isMockMode()) {
      throw new Error(
        "Mock mode enabled but no cached query plan found for this input",
      );
    }

    let planText: string | undefined;
    try {
      const client = getOpenAIClient();
      const completion = await client.chat.completions.create({
        messages: [
          { role: "system", content: getQueryPlanSystemPrompt() },
          { role: "user", content: userPrompt },
        ],
        model: resolveModel("query_planning"),
        max_tokens: 3000,
        temperature: 0.3, // è¨ˆç”»ç”Ÿæˆã¯ç¢ºå®šçš„ã«
        stream: false,
      });

      planText = completion.choices[0]?.message?.content?.trim();
      if (!planText) {
        throw new Error("No text in completion response");
      }
    } catch (error) {
      console.error("âŒ Planning error:", error);
      throw error;
    }

    // JSONãƒ‘ãƒ¼ã‚¹è©¦è¡Œ
    let planData;
    try {
      planData = JSON.parse(planText);
    } catch (parseError) {
      throw new Error(
        `Failed to parse LLM response as JSON: ${
          (parseError as Error).message
        }\nResponse: ${planText}`,
      );
    }

    // QueryPlanå½¢å¼ã«å¤‰æ›
    const plan: QueryPlan = {
      originalQuestion: userQuestion,
      subqueries: planData.subqueries || [userQuestion],
      entities: {
        speakers: planData.entities?.speakers || [],
        parties: planData.entities?.parties || [],
        topics: planData.entities?.topics || [],
        meetings: planData.entities?.meetings || [],
        positions: planData.entities?.positions || [],
        dateRange: planData.entities?.dateRange,
      },
      enabledStrategies: planData.enabledStrategies || ["vector"],
      confidence: planData.confidence || 0.5,
      estimatedComplexity: planData.estimatedComplexity || 2,
    };

    // QueryPlanã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    await this.cacheManager.save("query-planning", cacheInput, plan);

    return plan;
  }
}
