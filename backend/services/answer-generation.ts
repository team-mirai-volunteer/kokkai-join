// Answer generation service using Chain of Agents approach

import { Settings } from "npm:llamaindex";
import type { SpeechResult, SubSummaryResult } from "../types/kokkai.ts";
import {
	formatSpeechResultsForPrompt,
	createSubSummaryPrompt,
	createMidConsolidationPrompt,
	createFinalAnswerPrompt,
	createSimpleAnswerPrompt,
} from "../utils/prompt.ts";
import {
	CHAIN_OF_AGENTS_CHUNK_SIZE,
	CHAIN_OF_AGENTS_MIN_RESULTS,
	MID_CONSOLIDATION_CHUNK_SIZE,
	MID_CONSOLIDATION_THRESHOLD,
	CONTENT_PREVIEW_LENGTH,
} from "../config/constants.ts";

/**
 * Service responsible for generating answers using Chain of Agents approach
 */
export class AnswerGenerationService {
	/**
	 * Generate a sub-summary for a chunk of speech results
	 */
	private async generateSubSummary(
		chunk: SpeechResult[],
		chunkIndex: number,
		totalChunks: number,
		query: string,
	): Promise<SubSummaryResult> {
		const context = formatSpeechResultsForPrompt(chunk);
		const subPrompt = createSubSummaryPrompt(
			query,
			context,
			chunkIndex,
			totalChunks,
		);

		try {
			const response = await Settings.llm!.complete({ prompt: subPrompt });
			return {
				chunkIndex: chunkIndex + 1,
				summary: response.text,
				sourceCount: chunk.length,
			};
		} catch (error) {
			console.error(`âŒ Sub-summary ${chunkIndex + 1} failed:`, error);
			return {
				chunkIndex: chunkIndex + 1,
				summary: "è¦ç´„ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ",
				sourceCount: chunk.length,
			};
		}
	}

	/**
	 * Generate answer using Chain of Agents multi-stage summarization
	 */
	async generateAnswer(
		query: string,
		results: SpeechResult[],
	): Promise<string> {
		if (!Settings.llm) {
			throw new Error("LLM not initialized");
		}

		console.log(`\nğŸ¤– Generating answer using Chain of Agents...`);
		console.log(`ğŸ“Š Total results to process: ${results.length}`);

		// çµæœãŒå°‘ãªã„å ´åˆã¯å¾“æ¥ã®å‡¦ç†
		if (results.length <= CHAIN_OF_AGENTS_MIN_RESULTS) {
			return this.generateSimpleAnswer(query, results);
		}

		// Chain of Agents: ç™ºè¨€è€…ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦ã‹ã‚‰è¦ç´„å‡¦ç†
		// Step 0: ç™ºè¨€è€…ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆåŒä¸€ç™ºè¨€è€…ã®ç™ºè¨€ãŒæ··åœ¨ã—ãªã„ã‚ˆã†ã«ï¼‰
		const speakerGroups = new Map<string, SpeechResult[]>();
		for (const result of results) {
			const speakerKey = `${result.speaker}_${result.party}`;
			if (!speakerGroups.has(speakerKey)) {
				speakerGroups.set(speakerKey, []);
			}
			speakerGroups.get(speakerKey)!.push(result);
		}

		console.log(`ğŸ“Š Grouped into ${speakerGroups.size} speakers`);

		// å„ç™ºè¨€è€…ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ï¼ˆåŒä¸€ç™ºè¨€è€…ã®ç™ºè¨€ã‚’ã¾ã¨ã‚ã¦å‡¦ç†ï¼‰
		const chunks: SpeechResult[][] = [];
		for (const [_, speeches] of speakerGroups) {
			// ç™ºè¨€è€…ã”ã¨ã«ãƒãƒ£ãƒ³ã‚¯åŒ–
			for (let i = 0; i < speeches.length; i += CHAIN_OF_AGENTS_CHUNK_SIZE) {
				const chunk = speeches.slice(i, i + CHAIN_OF_AGENTS_CHUNK_SIZE);
				chunks.push(chunk);
			}
		}

		console.log(`ğŸ“¦ Split into ${chunks.length} chunks for processing`);

		// Step 1: å„ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦è¡Œå‡¦ç†ã§ã‚µãƒ–è¦ç´„
		console.log(`âš™ï¸ Step 1: Generating sub-summaries...`);
		const subSummaryPromises = chunks.map((chunk, chunkIndex) =>
			this.generateSubSummary(chunk, chunkIndex, chunks.length, query),
		);

		const subSummaries = await Promise.all(subSummaryPromises);
		console.log(`âœ… Generated ${subSummaries.length} sub-summaries`);

		// Step 2: ã‚µãƒ–è¦ç´„ãŒå¤šã„å ´åˆã¯ä¸­é–“çµ±åˆ
		let finalSummaries = subSummaries.map((s) => s.summary);
		if (subSummaries.length > MID_CONSOLIDATION_THRESHOLD) {
			console.log(`âš™ï¸ Step 2: Intermediate consolidation...`);
			const midChunkSize = MID_CONSOLIDATION_CHUNK_SIZE;
			const midSummaries: string[] = [];

			for (let i = 0; i < finalSummaries.length; i += midChunkSize) {
				const midChunk = finalSummaries.slice(i, i + midChunkSize);
				const midPrompt = createMidConsolidationPrompt(query, midChunk, i);

				try {
					const response = await Settings.llm!.complete({
						prompt: midPrompt,
					});
					midSummaries.push(response.text);
				} catch (error) {
					console.error(`âŒ Mid-level consolidation failed:`, error);
					midSummaries.push(midChunk.join("\n"));
				}
			}

			finalSummaries = midSummaries;
			console.log(
				`âœ… Consolidated to ${midSummaries.length} intermediate summaries`,
			);
		}

		// Step 3: æœ€çµ‚çµ±åˆã¨å›ç­”ç”Ÿæˆ
		console.log(`âš™ï¸ Step 3: Final answer generation...`);
		const finalContext = finalSummaries
			.map((s, idx) => `ã€è¦ç´„${idx + 1}ã€‘\n${s}`)
			.join("\n\n");

		const finalPrompt = createFinalAnswerPrompt(query, finalContext);

		try {
			const response = await Settings.llm.complete({ prompt: finalPrompt });
			console.log(`âœ… Final answer generated successfully`);
			return response.text;
		} catch (error) {
			console.error("âŒ Final answer generation error:", error);
			return this.generateSimpleAnswer(query, results);
		}
	}

	/**
	 * Generate simple answer (fallback method)
	 */
	async generateSimpleAnswer(
		query: string,
		results: SpeechResult[],
	): Promise<string> {
		if (!Settings.llm) {
			throw new Error("LLM not initialized");
		}

		const context = formatSpeechResultsForPrompt(results);
		const prompt = createSimpleAnswerPrompt(query, context);

		try {
			const response = await Settings.llm.complete({ prompt });
			return response.text;
		} catch (error) {
			console.error("âŒ LLM generation error:", error);
			return `æ¤œç´¢çµæœã«åŸºã¥ãæƒ…å ±:

${results
	.map(
		(result, index) =>
			`${index + 1}. ${result.speaker} (${result.party})
   æ—¥ä»˜: ${result.date}
   ä¼šè­°: ${result.meeting}
   å†…å®¹: ${result.content.substring(0, CONTENT_PREVIEW_LENGTH)}...
   å‡ºå…¸: ${result.url}
   é–¢é€£åº¦: ${result.score.toFixed(3)}`,
	)
	.join("\n\n")}`;
		}
	}
}
