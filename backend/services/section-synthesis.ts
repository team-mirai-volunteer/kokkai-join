import { getOpenAIClient } from "../config/openai.ts";
import type { DeepResearchSections, EvidenceRecord } from "../types/deepresearch.ts";
import { createSectionSynthesisPrompt, getSectionSynthesisSystemPrompt } from "../utils/prompt.ts";
import { AICacheManager } from "../utils/ai-cache-manager.ts";

/**
 * ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆã‚µãƒ¼ãƒ“ã‚¹ã€‚
 *
 * - å½¹å‰²: åé›†ã—ãŸ Evidence ã‚’æ ¹æ‹ ã¨ã—ã¦ã€å›ºå®šã‚¹ã‚­ãƒ¼ãƒã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³JSONã‚’LLMï¼ˆOpenAIçµŒç”±ï¼‰ã§ç”Ÿæˆã™ã‚‹ã€‚
 * - å¤±æ•—æ™‚: JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ã«ã™ã‚‹ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯è¡Œã‚ãªã„æ–¹é‡ï¼‰ã€‚
 * - ã‚­ãƒ£ãƒƒã‚·ãƒ¥: AIå¿œç­”ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã€ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å†åˆ©ç”¨å¯èƒ½ã€‚
 */
export class SectionSynthesisService {
  private cacheManager: AICacheManager;

  constructor(cacheManager?: AICacheManager) {
    this.cacheManager = cacheManager || new AICacheManager();
  }

  async synthesize(
    userQuery: string,
    asOfDate: string | undefined,
    evidences: EvidenceRecord[],
  ): Promise<DeepResearchSections> {
    const user = createSectionSynthesisPrompt(userQuery, asOfDate, evidences);
    const systemPrompt = getSectionSynthesisSystemPrompt();

    const cacheInput = {
      userQuery,
      asOfDate,
      evidencesCount: evidences.length,
      evidenceIds: evidences.map((e) => e.id),
      userPrompt: user,
      systemPrompt,
      model: "gpt-5",
    };

    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    const cachedResponse = await this.cacheManager.load<DeepResearchSections>(
      "section-synthesis",
      cacheInput,
    );

    if (cachedResponse) {
      console.log("ğŸ“‚ Using cached section synthesis");
      return cachedResponse;
    }

    // ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
    if (this.cacheManager.isMockMode()) {
      throw new Error(
        "Mock mode enabled but no cached section synthesis found for this input",
      );
    }

    const client = getOpenAIClient();
    const completion = await client.chat.completions.create({
      messages: [
        { role: "system", content: systemPrompt },
        { role: "user", content: user },
      ],
      model: "gpt-5",
      max_completion_tokens: 8000,
      stream: false,
    });

    const jsonText = completion.choices[0]?.message?.content?.trim();
    if (!jsonText) throw new Error("[SYN][llm] Empty synthesis response");

    try {
      const sections = JSON.parse(jsonText) as DeepResearchSections;

      // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
      await this.cacheManager.save("section-synthesis", cacheInput, sections);

      return sections;
    } catch (e) {
      const snippet = jsonText.slice(0, 400).replace(/\n/g, " ");
      throw new Error(
        `[SYN][llm-parse] Failed to parse JSON: ${(e as Error).message}; snippet="${snippet}..."`,
      );
    }
  }
}
