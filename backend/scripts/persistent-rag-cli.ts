#!/usr/bin/env -S deno run -A

import { load } from "@std/dotenv";
import { Settings } from "npm:llamaindex";
import { Ollama, OllamaEmbedding } from "npm:@llamaindex/ollama";
import { Pool } from "npm:pg";
import pgvector from "npm:pgvector/pg";

interface SpeechResult {
	speechId: string;
	speaker: string;
	party: string;
	date: string;
	meeting: string;
	content: string;
	url: string;
	score: number;
}

interface DatabaseRow {
	speech_id: string;
	speaker: string | null;
	speaker_group: string | null;
	date: string | null;
	meeting_name: string | null;
	speech_text: string | null;
	speech_url: string | null;
	similarity_score: string;
}

class PersistentKokkaiRAGCLI {
	private dbPool: Pool | null = null;

	async initialize(): Promise<void> {
		// ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
		await load({ export: true });

		const databaseUrl = Deno.env.get("DATABASE_URL");
		const ollamaBaseUrl =
			Deno.env.get("OLLAMA_BASE_URL") || "http://localhost:11434";

		if (!databaseUrl) {
			throw new Error("DATABASE_URL environment variable is required");
		}

		// Ollamaè¨­å®š
		try {
			Settings.embedModel = new OllamaEmbedding({
				model: "bge-m3",
				config: {
					host: ollamaBaseUrl,
				},
			});

			Settings.llm = new Ollama({
				model: "gpt-oss:20b",
				config: {
					host: ollamaBaseUrl,
				},
			});
		} catch (error) {
			throw new Error(
				`Failed to initialize Ollama: ${(error as Error).message}`,
			);
		}

		// ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«
		this.dbPool = new Pool({
			connectionString: databaseUrl,
			max: 10,
		});

		// pgvectorã‚¿ã‚¤ãƒ—ç™»éŒ²
		const client = await this.dbPool.connect();
		try {
			await pgvector.registerTypes(client);
		} finally {
			client.release();
		}

		console.log("ğŸš€ Persistent Kokkai RAG CLI initialized successfully");
	}

	async search(query: string, topK: number = 5): Promise<SpeechResult[]> {
		if (!this.dbPool || !Settings.embedModel) {
			throw new Error("Database or embedding model not initialized");
		}

		console.log(`ğŸ” Searching for: "${query}"`);

		try {
			// ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
			const queryEmbedding = await Settings.embedModel.getTextEmbedding(query);

			// ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œ
			const searchQuery = `
				SELECT 
					speech_id,
					speaker,
					speaker_group,
					date,
					meeting_name,
					speech_text,
					speech_url,
					(1 - (embedding <=> $1)) as similarity_score
				FROM kokkai_speech_embeddings
				WHERE embedding <=> $1 < 0.7
				ORDER BY embedding <=> $1
				LIMIT $2
			`;

			const result = await this.dbPool.query(searchQuery, [
				pgvector.toSql(queryEmbedding),
				topK,
			]);

			// çµæœã‚’SpeechResultå½¢å¼ã«å¤‰æ›
			const results: SpeechResult[] = result.rows.map((row: DatabaseRow) => ({
				speechId: row.speech_id,
				speaker: row.speaker || "æœªçŸ¥ã®è­°å“¡",
				party: row.speaker_group || "?",
				date: row.date || "2024-01-01",
				meeting: row.meeting_name || "?",
				content: row.speech_text || "",
				url: row.speech_url || "",
				score: parseFloat(row.similarity_score) || 0.0,
			}));

			return results;
		} catch (error) {
			console.error("âŒ Search error:", error);
			throw error;
		}
	}

	async generateAnswer(
		query: string,
		results: SpeechResult[],
	): Promise<string> {
		if (!Settings.llm) {
			throw new Error("LLM not initialized");
		}

		// æ¤œç´¢çµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦æ•´ç†
		const context = results
			.map(
				(result, index) =>
					`ã€ç™ºè¨€ ${index + 1}ã€‘
è­°å“¡: ${result.speaker} (${result.party})
æ—¥ä»˜: ${result.date}
ä¼šè­°: ${result.meeting}
å†…å®¹: ${result.content}
å‡ºå…¸: ${result.url}
é–¢é€£åº¦: ${result.score.toFixed(3)}
`,
			)
			.join("\n");

		const prompt = `ä»¥ä¸‹ã®å›½ä¼šè­°äº‹éŒ²ã‹ã‚‰ã€è³ªå•ã«å¯¾ã—ã¦æ­£ç¢ºã§è©³ç´°ãªå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

è³ªå•: ${query}

å›½ä¼šè­°äº‹éŒ²:
${context}

å›ç­”è¦ä»¶:
1. ç™ºè¨€è€…åã¨æ‰€å±æ”¿å…šã‚’æ˜è¨˜ã™ã‚‹
2. ç™ºè¨€ã®æ—¥ä»˜ã¨ä¼šè­°åã‚’å«ã‚ã‚‹
3. å…·ä½“çš„ãªå†…å®¹ã‚’å¼•ç”¨ã™ã‚‹
4. å‡ºå…¸URLã‚’æç¤ºã™ã‚‹
5. è¤‡æ•°ã®ç™ºè¨€ãŒã‚ã‚‹å ´åˆã¯æ¯”è¼ƒãƒ»æ•´ç†ã™ã‚‹
6. äº‹å®Ÿã«åŸºã¥ã„ã¦å›ç­”ã—ã€æ¨æ¸¬ã¯é¿ã‘ã‚‹

å›ç­”:`;

		try {
			const response = await Settings.llm.complete({ prompt });
			return response.text;
		} catch (error) {
			console.error("âŒ LLM generation error:", error);
			return `æ¤œç´¢çµæœã«åŸºã¥ãæƒ…å ±:

${results
	.map(
		(result, index) =>
			`${index + 1}. ${result.speaker} (${result.party})
   æ—¥ä»˜: ${result.date}
   ä¼šè­°: ${result.meeting}
   å†…å®¹: ${result.content.substring(0, 300)}...
   å‡ºå…¸: ${result.url}
   é–¢é€£åº¦: ${result.score.toFixed(3)}`,
	)
	.join("\n\n")}`;
		}
	}

	async close(): Promise<void> {
		if (this.dbPool) {
			await this.dbPool.end();
			console.log("ğŸ“Š Database connection closed");
		}
	}

	formatResults(results: SpeechResult[]): void {
		console.log(`\nğŸ“‹ Found ${results.length} results:\n`);

		results.forEach((result, index) => {
			console.log(`--- Result ${index + 1} ---`);
			console.log(`ğŸ‘¤ Speaker: ${result.speaker} (${result.party})`);
			console.log(`ğŸ“… Date: ${result.date}`);
			console.log(`ğŸ›ï¸ Meeting: ${result.meeting}`);
			console.log(`â­ Score: ${result.score.toFixed(3)}`);
			console.log(`ğŸ”— URL: ${result.url}`);
			console.log(
				`ğŸ’¬ Content: ${result.content.substring(0, 300)}${
					result.content.length > 300 ? "..." : ""
				}`,
			);
			console.log("");
		});
	}

	async getStats(): Promise<void> {
		if (!this.dbPool) return;

		try {
			const totalResult = await this.dbPool.query(
				'SELECT COUNT(*) as count FROM "Speech"',
			);
			const embeddedResult = await this.dbPool.query(
				"SELECT COUNT(*) as count FROM kokkai_speech_embeddings",
			);

			console.log("\nğŸ“Š Database Statistics:");
			console.log(`Total speeches: ${totalResult.rows[0].count}`);
			console.log(`Embedded speeches: ${embeddedResult.rows[0].count}`);
			const percentage =
				(embeddedResult.rows[0].count / totalResult.rows[0].count) * 100;
			console.log(`Embedded percentage: ${percentage.toFixed(1)}%`);
		} catch (error) {
			console.error("Failed to get stats:", error);
		}
	}
}

async function main(): Promise<void> {
	const args = Deno.args;

	if (args.length === 0) {
		console.error(
			'âŒ Usage: deno run -A scripts/persistent-rag-cli.ts "æ¤œç´¢ã‚¯ã‚¨ãƒª"',
		);
		console.error(
			'   Example: deno run -A scripts/persistent-rag-cli.ts "å²¸ç”°ç·ç†ã®é˜²è¡›è²»ã«ã¤ã„ã¦"',
		);
		Deno.exit(1);
	}

	const query = args.join(" ");
	const ragCli = new PersistentKokkaiRAGCLI();

	try {
		await ragCli.initialize();
		await ragCli.getStats();

		// ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œ
		const results = await ragCli.search(query);

		if (results.length === 0) {
			console.log("âŒ No relevant speeches found.");
			return;
		}

		// æ¤œç´¢çµæœè¡¨ç¤º
		ragCli.formatResults(results);

		// LLMã«ã‚ˆã‚‹å›ç­”ç”Ÿæˆ
		console.log("ğŸ¤– Generating AI answer...\n");
		const answer = await ragCli.generateAnswer(query, results);

		console.log("â•".repeat(80));
		console.log("ğŸ¯ AI-Generated Answer:");
		console.log("â•".repeat(80));
		console.log(answer);
		console.log("â•".repeat(80));
	} catch (error) {
		console.error("âŒ Error:", (error as Error).message);
		Deno.exit(1);
	} finally {
		await ragCli.close();
	}
}

if (import.meta.main) {
	await main();
}
